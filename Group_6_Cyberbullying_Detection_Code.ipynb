{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group_6_Cyberbullying_Detection_Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Detection Of Cyberbullying on Reddit Comments"
      ],
      "metadata": {
        "id": "g7eJ6dNK4-4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running prerequisite code for connecting to Google Drive, setting up environment variables and initializing Spark"
      ],
      "metadata": {
        "id": "pzbI4t075DHK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKhb5oPPAHj4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "f80eabcc-e278-482b-9276-102d89ac7b14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://86b55c4dd9e0:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark  \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.0-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifying the path of reddit comments and abusive word list dataset"
      ],
      "metadata": {
        "id": "SLMq-83D5HOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/gdrive/MyDrive/Set_50K.csv'\n",
        "\n",
        "words = '/content/gdrive/MyDrive/list.csv'"
      ],
      "metadata": {
        "id": "lpEFgoTvAZoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Storing the reddit comments in a pyspark dataframe and the abusive words in a RDD"
      ],
      "metadata": {
        "id": "eEnojk7h5Ja3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.options(header='True', inferSchema='True', delimiter=',').csv(path)\n",
        "wordsRDD = sc.textFile(words)"
      ],
      "metadata": {
        "id": "9DI6oUGAAhCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWsTJu3JAj7k",
        "outputId": "74105211-1cb6-425e-bbe9-ecacbf8ac1ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- subreddit: string (nullable = true)\n",
            " |-- body: string (nullable = true)\n",
            " |-- controversiality: string (nullable = true)\n",
            " |-- score: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding row numbers to the dataframe"
      ],
      "metadata": {
        "id": "msVYE4yZ5MZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "df = df.withColumn(\"id\",monotonically_increasing_id())\n",
        "df=df.na.drop()\n"
      ],
      "metadata": {
        "id": "foBrotMdzjgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLoIO37J0m80",
        "outputId": "5b579838-9ed1-4566-801d-befc332039f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------------+----------------+-----+---+\n",
            "|  subreddit|                body|controversiality|score| id|\n",
            "+-----------+--------------------+----------------+-----+---+\n",
            "|*I am a bot| and this action ...|               0|    1|  1|\n",
            "|        aww|Dont squeeze her ...|               0|   19|  2|\n",
            "|     gaming|It's pretty well ...|               0|    3|  3|\n",
            "|       news|You know we have ...|               0|   10|  4|\n",
            "|   politics|Yes, there is a d...|               0|    1|  5|\n",
            "+-----------+--------------------+----------------+-----+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordsRDD.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sjItLfeCW_9",
        "outputId": "4f6d4a8c-c9dd-4e64-db0a-195c37e67905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['69', '@55', '@ssfcker', '@ssfucker', '@ssfvcker']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = wordsRDD.collect()"
      ],
      "metadata": {
        "id": "9Gf2XKN-Amg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "## I. Tokenization\n",
        "1) Converting comments to lower case"
      ],
      "metadata": {
        "id": "bXhFmHjTB5sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lower, col\n",
        "df1 = df.withColumn(\"lower_body\",lower(col(\"body\"))).select(\"subreddit\",\"body\",\"lower_body\")\n",
        "df1=df1.na.drop()\n",
        "df1.show()"
      ],
      "metadata": {
        "id": "BBXddeY5BG-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6303939-43f2-4548-a7ad-6864e86cc098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|           subreddit|                body|          lower_body|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|         *I am a bot| and this action ...| and this action ...|\n",
            "|                 aww|Dont squeeze her ...|dont squeeze her ...|\n",
            "|              gaming|It's pretty well ...|it's pretty well ...|\n",
            "|                news|You know we have ...|you know we have ...|\n",
            "|            politics|Yes, there is a d...|yes, there is a d...|\n",
            "|           dankmemes|Please let this b...|please let this b...|\n",
            "| relationship_advice|I would be less w...|i would be less w...|\n",
            "|                 nba|REPORT: Water is ...|report: water is ...|\n",
            "|           worldnews|How many millions...|how many millions...|\n",
            "|                 aww|What an amazing t...|what an amazing t...|\n",
            "|           AskReddit|Like a giant turd...|like a giant turd...|\n",
            "|              gaming|Why would we want...|why would we want...|\n",
            "|                 nba|*michael jackson ...|*michael jackson ...|\n",
            "|       AmItheAsshole|I didn't til I wa...|i didn't til i wa...|\n",
            "|       SquaredCircle|Twitter totally d...|twitter totally d...|\n",
            "|          The_Donald|Man it felt good ...|man it felt good ...|\n",
            "|           AskReddit|So you'd rather w...|so you'd rather w...|\n",
            "|       AmItheAsshole|Yup. It is, and i...|yup. it is, and i...|\n",
            "|I laughed when I ...| because just tod...| because just tod...|\n",
            "|I have 0 interest...|  and I love my wife|  and i love my wife|\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLPGLEiHAo1m",
        "outputId": "756f86e8-fe51-4b2b-b93e-dabef9a36fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Splitting the comments into a list of words"
      ],
      "metadata": {
        "id": "6n_sEloRB80d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split\n",
        "df1 = df1.withColumn(\"split\", split(\"lower_body\", \" \")).select(\"subreddit\",\"body\",\"lower_body\",\"split\")\n",
        "df1=df1.na.drop()\n",
        "df1.show(5)"
      ],
      "metadata": {
        "id": "kdCcxtljAxIB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cded5691-89dd-4604-ca6a-15b0fd4162c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------------+--------------------+--------------------+\n",
            "|  subreddit|                body|          lower_body|               split|\n",
            "+-----------+--------------------+--------------------+--------------------+\n",
            "|*I am a bot| and this action ...| and this action ...|[, and, this, act...|\n",
            "|        aww|Dont squeeze her ...|dont squeeze her ...|[dont, squeeze, h...|\n",
            "|     gaming|It's pretty well ...|it's pretty well ...|[it's, pretty, we...|\n",
            "|       news|You know we have ...|you know we have ...|[you, know, we, h...|\n",
            "|   politics|Yes, there is a d...|yes, there is a d...|[yes,, there, is,...|\n",
            "+-----------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "body_none = df1.select('split').rdd.flatMap(list)\n",
        "body = body_none.map(lambda y: y if y is not None else '')"
      ],
      "metadata": {
        "id": "L7rowA-jA0aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II. Lemmatization"
      ],
      "metadata": {
        "id": "ULmD0cEYCAJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "body_lem = body.map(lambda x: [ lemmatizer.lemmatize(i) for i in x])\n"
      ],
      "metadata": {
        "id": "lM500aVpBQo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III. Stop-Words Removal"
      ],
      "metadata": {
        "id": "u2vnlyoQCEnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5C3IGj7BWLl",
        "outputId": "08b26e30-0ac3-4c9f-9cda-fddd7f00e83c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = set(stopwords.words('english'))\n",
        "body_sr = body_lem.map(lambda x: [i for i in x if i not in stop])\n",
        "body_sr.take(2)"
      ],
      "metadata": {
        "id": "HCEnSDavBcLV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36960834-3f65-48bc-cef7-284d9cfa942e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['',\n",
              "  'action',\n",
              "  'wa',\n",
              "  'performed',\n",
              "  'automatically.',\n",
              "  'please',\n",
              "  '[contact',\n",
              "  'moderator',\n",
              "  'subreddit](/message/compose/?to=/r/gameofthrones)',\n",
              "  'question',\n",
              "  'concerns.*\"'],\n",
              " ['dont', 'squeeze', 'massive', 'hand,', 'mean', 'giant.']]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting RDDs to dataframe and merging them with the original dataframe"
      ],
      "metadata": {
        "id": "C_uEYR6_5fTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "body_sr_collect = spark.createDataFrame(\n",
        "    body_sr.zipWithIndex(),\n",
        "    StructType([\n",
        "        StructField(\"words\", ArrayType(StringType())),\n",
        "        StructField(\"id_words\", LongType())\n",
        "    ])\n",
        ")"
      ],
      "metadata": {
        "id": "7neiM_9m02_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "body_sr_collect.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na8JeI5t08L_",
        "outputId": "691e0bc5-e727-48fa-a0fe-983e18c6b981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------+\n",
            "|               words|id_words|\n",
            "+--------------------+--------+\n",
            "|[, action, wa, pe...|       0|\n",
            "|[dont, squeeze, m...|       1|\n",
            "|[pretty, well, kn...|       2|\n",
            "|[know, law, curre...|       3|\n",
            "|[yes,, difference...|       4|\n",
            "+--------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xx = df.join(body_sr_collect,body_sr_collect.id_words == df.id,\"inner\")\n",
        "xx=xx.drop(\"id\")"
      ],
      "metadata": {
        "id": "ttYc4PJK1bDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xx.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSjdCDNE1bI5",
        "outputId": "33f418d0-3c4f-4be3-d285-c8b6f90b9709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------------+----------------+-----+--------------------+--------+\n",
            "|  subreddit|                body|controversiality|score|               words|id_words|\n",
            "+-----------+--------------------+----------------+-----+--------------------+--------+\n",
            "|*I am a bot| and this action ...|               0|    1|[dont, squeeze, m...|       1|\n",
            "|        aww|Dont squeeze her ...|               0|   19|[pretty, well, kn...|       2|\n",
            "|     gaming|It's pretty well ...|               0|    3|[know, law, curre...|       3|\n",
            "|       news|You know we have ...|               0|   10|[yes,, difference...|       4|\n",
            "|   politics|Yes, there is a d...|               0|    1|[please, let, bec...|       5|\n",
            "+-----------+--------------------+----------------+-----+--------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counting the number of abusive words in  a comment and counting the total number of words in a comment after stop-words removal"
      ],
      "metadata": {
        "id": "6T_G5Cce5h4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bully = body_sr.map(lambda y: y if y is not None else '')\n",
        "tot_count = bully.map(lambda x: len(x))\n",
        "bully2 = bully.map(lambda y: [x for x in y if x in words])"
      ],
      "metadata": {
        "id": "3QCCdJ8eBljY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully2.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdi0pBDpC6H6",
        "outputId": "a2a2507e-5b91-4775-b56f-08ff77128d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[], [], [], [], []]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "bully_word_collect = spark.createDataFrame(\n",
        "    bully2.zipWithIndex(),\n",
        "    StructType([\n",
        "        StructField(\"BullyWords\",ArrayType(StringType())),\n",
        "        StructField(\"id_bull\", LongType())\n",
        "    ])\n",
        ")\n",
        "bully_word_collect.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MskjfH7o99sa",
        "outputId": "a9866077-2992-4ba1-ad5e-e599a02e2583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+\n",
            "|BullyWords|id_bull|\n",
            "+----------+-------+\n",
            "|        []|      0|\n",
            "|        []|      1|\n",
            "|        []|      2|\n",
            "|        []|      3|\n",
            "|        []|      4|\n",
            "+----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bully_count = bully2.map(lambda x: len(x))"
      ],
      "metadata": {
        "id": "4qpKW6W5BvnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_count.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC2A-lba6Q-2",
        "outputId": "ca1b9272-5257-4784-dc30-9885742bab6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "bully_count_collect = spark.createDataFrame(\n",
        "    bully_count.zipWithIndex(),\n",
        "    StructType([\n",
        "        StructField(\"Count\", LongType()),\n",
        "        StructField(\"id_cnt\", LongType())\n",
        "    ])\n",
        ")"
      ],
      "metadata": {
        "id": "z9Vl6wO04J1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tot_count.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qhhG-gv6B3t",
        "outputId": "871383ee-2cbd-4dd0-a377-f2d27aecb523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[11, 6, 43, 9, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "tot_count_collect = spark.createDataFrame(\n",
        "    tot_count.zipWithIndex(),\n",
        "    StructType([\n",
        "        StructField(\"Total_Count\", LongType()),\n",
        "        StructField(\"id_tot\", LongType())\n",
        "    ])\n",
        ")"
      ],
      "metadata": {
        "id": "Rn4SPe7y4Rib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jj = bully_count_collect.join(bully_word_collect,bully_count_collect.id_cnt == bully_word_collect.id_bull,\"inner\")\n",
        "jj.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwuS3PMI-y5E",
        "outputId": "3b9d81ff-60f0-4a2e-8126-094f80e08db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+-------+\n",
            "|Count|id_cnt|BullyWords|id_bull|\n",
            "+-----+------+----------+-------+\n",
            "|    0|    26|        []|     26|\n",
            "|    0|    29|        []|     29|\n",
            "|    0|   474|        []|    474|\n",
            "|    0|   964|        []|    964|\n",
            "|    0|  1677|        []|   1677|\n",
            "+-----+------+----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "va = jj.join(tot_count_collect,tot_count_collect.id_tot == jj.id_cnt,\"inner\")\n",
        "va.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVXuaGYk4mQE",
        "outputId": "1643824e-0bb6-45f9-8d1f-f6e01c2e519c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+-------+-----------+------+\n",
            "|Count|id_cnt|BullyWords|id_bull|Total_Count|id_tot|\n",
            "+-----+------+----------+-------+-----------+------+\n",
            "|    0|    26|        []|     26|         10|    26|\n",
            "|    0|    29|        []|     29|          3|    29|\n",
            "|    0|   474|        []|    474|          8|   474|\n",
            "|    0|   964|        []|    964|          8|   964|\n",
            "|    0|  1677|        []|   1677|          6|  1677|\n",
            "+-----+------+----------+-------+-----------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yy = xx.join(va,xx.id_words == va.id_tot,\"inner\")\n",
        "yy.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IKnK3l549Dv",
        "outputId": "c1d8803f-1861-4e49-d080-db5e4e141e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------------+----------------+-----+--------------------+--------+-----+------+----------+-------+-----------+------+\n",
            "|    subreddit|                body|controversiality|score|               words|id_words|Count|id_cnt|BullyWords|id_bull|Total_Count|id_tot|\n",
            "+-------------+--------------------+----------------+-----+--------------------+--------+-----+------+----------+-------+-----------+------+\n",
            "|          nba|[the only reason ...|               0|   -5|[invest, lot, arc...|      29|    0|    29|        []|     29|          3|    29|\n",
            "|       gaming|i don’t know who ...|               0|    3|[worry, it., ever...|     474|    0|   474|        []|    474|          8|   474|\n",
            "|marvelstudios|She's saying that...|               0|    3|[arya, wa, hand, ...|     964|    0|   964|        []|    964|          8|   964|\n",
            "|SquaredCircle|OH MYNGOD MICHAEL...|               0|    2|[ally, knew, secr...|    1677|    0|  1677|        []|   1677|          6|  1677|\n",
            "| MortalKombat|Yeah I think mine...|               0|    5|[lol, okkk, learn...|    1697|    0|  1697|        []|   1697|          6|  1697|\n",
            "+-------------+--------------------+----------------+-----+--------------------+--------+-----+------+----------+-------+-----------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yy=yy.drop(\"id_tot\", \"id_cnt\")"
      ],
      "metadata": {
        "id": "4OtJCTUuQ84V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yy=yy.na.drop()"
      ],
      "metadata": {
        "id": "4sc2OU_I4-Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset with total count and count of abusive words for each comment"
      ],
      "metadata": {
        "id": "Suj4IOox5mk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yy.show(5)"
      ],
      "metadata": {
        "id": "3yqwo4DpByNG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00cb638e-6b03-4c14-f7fc-fbc70cc8fc5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------------+----------------+-----+--------------------+--------+-----+----------+-------+-----------+\n",
            "|    subreddit|                body|controversiality|score|               words|id_words|Count|BullyWords|id_bull|Total_Count|\n",
            "+-------------+--------------------+----------------+-----+--------------------+--------+-----+----------+-------+-----------+\n",
            "|          nba|[the only reason ...|               0|   -5|[invest, lot, arc...|      29|    0|        []|     29|          3|\n",
            "|       gaming|i don’t know who ...|               0|    3|[worry, it., ever...|     474|    0|        []|    474|          8|\n",
            "|marvelstudios|She's saying that...|               0|    3|[arya, wa, hand, ...|     964|    0|        []|    964|          8|\n",
            "|SquaredCircle|OH MYNGOD MICHAEL...|               0|    2|[ally, knew, secr...|    1677|    0|        []|   1677|          6|\n",
            "| MortalKombat|Yeah I think mine...|               0|    5|[lol, okkk, learn...|    1697|    0|        []|   1697|          6|\n",
            "+-------------+--------------------+----------------+-----+--------------------+--------+-----+----------+-------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating the offensiveness Proportion for each comment"
      ],
      "metadata": {
        "id": "ZNB9_QLT5oqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import row_number,lit\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "df_qq = yy.withColumn(\"Proportion\",F.col(\"Count\")/F.col(\"Total_Count\"))\n",
        "df_qq=df_qq.na.drop()\n",
        "df_qq.show(5)"
      ],
      "metadata": {
        "id": "s2mgbDfsDAeE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1178b54-4cd2-4adf-d584-93a3cc6a9b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------------+----------------+-----+--------------------+--------+-----+----------+-------+-----------+----------+\n",
            "|    subreddit|                body|controversiality|score|               words|id_words|Count|BullyWords|id_bull|Total_Count|Proportion|\n",
            "+-------------+--------------------+----------------+-----+--------------------+--------+-----+----------+-------+-----------+----------+\n",
            "|          nba|[the only reason ...|               0|   -5|[invest, lot, arc...|      29|    0|        []|     29|          3|       0.0|\n",
            "|       gaming|i don’t know who ...|               0|    3|[worry, it., ever...|     474|    0|        []|    474|          8|       0.0|\n",
            "|marvelstudios|She's saying that...|               0|    3|[arya, wa, hand, ...|     964|    0|        []|    964|          8|       0.0|\n",
            "|SquaredCircle|OH MYNGOD MICHAEL...|               0|    2|[ally, knew, secr...|    1677|    0|        []|   1677|          6|       0.0|\n",
            "| MortalKombat|Yeah I think mine...|               0|    5|[lol, okkk, learn...|    1697|    0|        []|   1697|          6|       0.0|\n",
            "+-------------+--------------------+----------------+-----+--------------------+--------+-----+----------+-------+-----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis for each comment"
      ],
      "metadata": {
        "id": "HDaGJ8z0CXEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install vaderSentiment\n"
      ],
      "metadata": {
        "id": "-M6MXrFycuf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac865795-48fd-4356-8f65-fbe666ea4ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 28.6 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40 kB 23.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 24.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.10.8)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "id": "dAqLLyo2cWEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "my_udf_sent = udf(lambda x: SentimentIntensityAnalyzer().polarity_scores(x))\n",
        "\n",
        "df_sent = df_qq.withColumn(\"Sent\",my_udf_sent(col(\"body\")))\n",
        "df_sent.show(5)"
      ],
      "metadata": {
        "id": "4vzswREGdSdn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0218cc0-e638-453f-bd4b-4988bcaaf2c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------------+----------------+-----+--------------------+--------+-----+----------+-------+-----------+----------+--------------------+\n",
            "|    subreddit|                body|controversiality|score|               words|id_words|Count|BullyWords|id_bull|Total_Count|Proportion|                Sent|\n",
            "+-------------+--------------------+----------------+-----+--------------------+--------+-----+----------+-------+-----------+----------+--------------------+\n",
            "|          nba|[the only reason ...|               0|   -5|[invest, lot, arc...|      29|    0|        []|     29|          3|       0.0|{neg=0.053, pos=0...|\n",
            "|       gaming|i don’t know who ...|               0|    3|[worry, it., ever...|     474|    0|        []|    474|          8|       0.0|{neg=0.0, pos=0.1...|\n",
            "|marvelstudios|She's saying that...|               0|    3|[arya, wa, hand, ...|     964|    0|        []|    964|          8|       0.0|{neg=0.036, pos=0...|\n",
            "|SquaredCircle|OH MYNGOD MICHAEL...|               0|    2|[ally, knew, secr...|    1677|    0|        []|   1677|          6|       0.0|{neg=0.0, pos=0.0...|\n",
            "| MortalKombat|Yeah I think mine...|               0|    5|[lol, okkk, learn...|    1697|    0|        []|   1697|          6|       0.0|{neg=0.0, pos=0.1...|\n",
            "+-------------+--------------------+----------------+-----+--------------------+--------+-----+----------+-------+-----------+----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the compound score through regular expressions"
      ],
      "metadata": {
        "id": "3IjFBeHzDOAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split,regexp_extract\n",
        "\n",
        "df_sent=df_sent.withColumn(\"compound\", regexp_extract(\"Sent\", \"compound=(.*),\", 1))\n",
        "df_sent.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXei4Drcml01",
        "outputId": "c1cceaab-52b3-4fc7-f96f-c1f781babe88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------------+----------------+-----+--------------------+--------+-----+----------+-------+-----------+----------+--------------------+--------+\n",
            "|    subreddit|                body|controversiality|score|               words|id_words|Count|BullyWords|id_bull|Total_Count|Proportion|                Sent|compound|\n",
            "+-------------+--------------------+----------------+-----+--------------------+--------+-----+----------+-------+-----------+----------+--------------------+--------+\n",
            "|          nba|[the only reason ...|               0|   -5|[invest, lot, arc...|      29|    0|        []|     29|          3|       0.0|{neg=0.053, pos=0...|  0.0258|\n",
            "|       gaming|i don’t know who ...|               0|    3|[worry, it., ever...|     474|    0|        []|    474|          8|       0.0|{neg=0.0, pos=0.1...|  0.5023|\n",
            "|marvelstudios|She's saying that...|               0|    3|[arya, wa, hand, ...|     964|    0|        []|    964|          8|       0.0|{neg=0.036, pos=0...|  0.5859|\n",
            "|SquaredCircle|OH MYNGOD MICHAEL...|               0|    2|[ally, knew, secr...|    1677|    0|        []|   1677|          6|       0.0|{neg=0.0, pos=0.0...|     0.0|\n",
            "| MortalKombat|Yeah I think mine...|               0|    5|[lol, okkk, learn...|    1697|    0|        []|   1697|          6|       0.0|{neg=0.0, pos=0.1...|   0.296|\n",
            "+-------------+--------------------+----------------+-----+--------------------+--------+-----+----------+-------+-----------+----------+--------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import DecimalType\n",
        "df_sent=df_sent.withColumn(\"compound\", df_sent[\"compound\"].cast(DecimalType(20,4)))\n",
        "df_sent.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsdFeLHcmwei",
        "outputId": "c724250c-301a-4eea-b1b4-c2ac5abad38d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------------+----------------+-----+--------------------+--------+-----+----------+-------+-----------+----------+--------------------+--------+\n",
            "|    subreddit|                body|controversiality|score|               words|id_words|Count|BullyWords|id_bull|Total_Count|Proportion|                Sent|compound|\n",
            "+-------------+--------------------+----------------+-----+--------------------+--------+-----+----------+-------+-----------+----------+--------------------+--------+\n",
            "|          nba|[the only reason ...|               0|   -5|[invest, lot, arc...|      29|    0|        []|     29|          3|       0.0|{neg=0.053, pos=0...|  0.0258|\n",
            "|       gaming|i don’t know who ...|               0|    3|[worry, it., ever...|     474|    0|        []|    474|          8|       0.0|{neg=0.0, pos=0.1...|  0.5023|\n",
            "|marvelstudios|She's saying that...|               0|    3|[arya, wa, hand, ...|     964|    0|        []|    964|          8|       0.0|{neg=0.036, pos=0...|  0.5859|\n",
            "|SquaredCircle|OH MYNGOD MICHAEL...|               0|    2|[ally, knew, secr...|    1677|    0|        []|   1677|          6|       0.0|{neg=0.0, pos=0.0...|  0.0000|\n",
            "| MortalKombat|Yeah I think mine...|               0|    5|[lol, okkk, learn...|    1697|    0|        []|   1697|          6|       0.0|{neg=0.0, pos=0.1...|  0.2960|\n",
            "+-------------+--------------------+----------------+-----+--------------------+--------+-----+----------+-------+-----------+----------+--------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Labeling Algorithm"
      ],
      "metadata": {
        "id": "FrMWZjt_5xQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, col, when\n",
        "\n",
        "df_sent = df_sent.withColumn(\n",
        "    'SentLab',\n",
        "     when((col(\"compound\").between(0.05, 1)) & col(\"Proportion\").between(0.25, 1), 0)\\\n",
        "    .when((col(\"compound\").between(-0.05, 1)) & col('Proportion').between(0,0.25), 0)\\\n",
        "    .when((col(\"compound\").between(-1, -0.05)) & col('Proportion').between(0.25,1), 1)\\\n",
        "    .when((col(\"compound\").between(-0.05, 0.05)) & col('Proportion').between(0.5,1), 1)\\\n",
        "    .otherwise(0)\n",
        ")\n",
        "df_sent.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UduzGdDtsxlp",
        "outputId": "e0607671-d20b-485f-dce5-0ecf23c33714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+----------+-------+-----------+----------+--------------------+--------+-------+\n",
            "|           subreddit|                body|    controversiality|               score|               words|id_words|Count|BullyWords|id_bull|Total_Count|Proportion|                Sent|compound|SentLab|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+----------+-------+-----------+----------+--------------------+--------+-------+\n",
            "|                 nba|[the only reason ...|                   0|                  -5|[invest, lot, arc...|      29|    0|        []|     29|          3|       0.0|{neg=0.053, pos=0...|  0.0258|      0|\n",
            "|              gaming|i don’t know who ...|                   0|                   3|[worry, it., ever...|     474|    0|        []|    474|          8|       0.0|{neg=0.0, pos=0.1...|  0.5023|      0|\n",
            "|       marvelstudios|She's saying that...|                   0|                   3|[arya, wa, hand, ...|     964|    0|        []|    964|          8|       0.0|{neg=0.036, pos=0...|  0.5859|      0|\n",
            "|       SquaredCircle|OH MYNGOD MICHAEL...|                   0|                   2|[ally, knew, secr...|    1677|    0|        []|   1677|          6|       0.0|{neg=0.0, pos=0.0...|  0.0000|      0|\n",
            "|        MortalKombat|Yeah I think mine...|                   0|                   5|[lol, okkk, learn...|    1697|    0|        []|   1697|          6|       0.0|{neg=0.0, pos=0.1...|  0.2960|      0|\n",
            "|           AskReddit|He probably also ...|                   0|                   1|[kind, reminds, v...|    1950|    0|        []|   1950|         31|       0.0|{neg=0.367, pos=0...| -0.4404|      0|\n",
            "|                 nfl|Troy Polamalu. Re...|                   0|                   9|[scene, wa, way, ...|    2040|    0|        []|   2040|         11|       0.0|{neg=0.1, pos=0.2...|  0.5859|      0|\n",
            "|            politics|Yeah couldn’t agr...|                   0|                   4|[doe, last, day, ...|    2214|    0|        []|   2214|         14|       0.0|{neg=0.0, pos=0.1...|  0.5719|      0|\n",
            "|The point is that...| men and women al...| they didn't even...| and after they h...|[probably, emotio...|    2453|    0|        []|   2453|          9|       0.0|{neg=0.0, pos=0.0...|  0.0000|      0|\n",
            "|               funny|How is this unori...|                   0|                   0|[, action, wa, pe...|    2529|    0|        []|   2529|         11|       0.0|{neg=0.0, pos=0.0...|  0.0000|      0|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+----------+-------+-----------+----------+--------------------+--------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performing undersampling to tackle the class imbalance problem"
      ],
      "metadata": {
        "id": "1E-B_ilU50rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, explode, array, lit\n",
        "\n",
        "\n",
        "major_df = df_sent.filter(col(\"SentLab\") == 0)\n",
        "minor_df = df_sent.filter(col(\"SentLab\") == 1)\n",
        "ratio = int(major_df.count()/minor_df.count())\n",
        "print(\"ratio: {}\".format(ratio))\n",
        "\n",
        "sampled_majority_df = major_df.sample(withReplacement=False, fraction=1/ratio, seed=1)\n",
        "combined_df_2 = sampled_majority_df.unionAll(minor_df)\n",
        "# combined_df_2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCXW5bZgt2Vd",
        "outputId": "d1bb11c6-17f9-4105-e6a1-a994950ef4f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ratio: 404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "minor_df = combined_df_2.filter(col(\"SentLab\") == 1)\n"
      ],
      "metadata": {
        "id": "9haFpv22yV05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1/ratio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBfQuEElwirN",
        "outputId": "e8ea1c7b-569b-4f00-befa-54e848a4457d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0024752475247524753"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning"
      ],
      "metadata": {
        "id": "4FoM2bOU52nl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPKrcnby5CTT"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel,LogisticRegressionWithSGD\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes, GBTClassifier\n",
        "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel,LogisticRegressionWithSGD\n",
        "from pyspark.mllib.regression import LabeledPoint\n",
        "from pyspark.ml import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-test split"
      ],
      "metadata": {
        "id": "Kd-qG3IT56Bl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAuJjFmzN1UC"
      },
      "outputs": [],
      "source": [
        "dfFin = combined_df_2.selectExpr(\"body as text\", \"SentLab as label\")\n",
        "(trainingData, testData) = dfFin.randomSplit([0.8, 0.2])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering"
      ],
      "metadata": {
        "id": "drsrKIFA58B-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wC1fJ_dUnDA"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "def FeatureEng(data):\n",
        "  sentData = data\n",
        "#Tokenizing\n",
        "  tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "  wordsData = tokenizer.transform(sentData)\n",
        "#Hashing\n",
        "  hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=25)\n",
        "  featData = hashingTF.transform(wordsData)\n",
        "#IDF\n",
        "  idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "  idfModel = idf.fit(featData)\n",
        "  scaledData = idfModel.transform(featData)\n",
        " \n",
        "  scaledData=scaledData.withColumn(\"label\", scaledData[\"label\"].cast(IntegerType()))\n",
        "  return scaledData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcKHosnFZ6Le"
      },
      "outputs": [],
      "source": [
        "train = FeatureEng(trainingData)\n",
        "test = FeatureEng(testData)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "JZ35Widn5-2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgFPZOUCUnHL"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(maxIter=100)\n",
        "lrModel = lr.fit(train)\n",
        "predictions = lrModel.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "Mevaluator = MulticlassClassificationEvaluator()\n",
        "Mevaluator.setPredictionCol(\"prediction\")\n",
        "acc = Mevaluator.evaluate(predictions, {Mevaluator.metricName: \"accuracy\"})\n",
        "f1 = Mevaluator.evaluate(predictions, {Mevaluator.metricName: \"f1\"})\n",
        "pre = Mevaluator.evaluate(predictions, {Mevaluator.metricName: \"precisionByLabel\"})\n",
        "rec = Mevaluator.evaluate(predictions, {Mevaluator.metricName: \"recallByLabel\"})\n",
        "# auc = Mevaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
        "\n",
        "print('Precision: %0.3f' % pre)\n",
        "print('Recall: %0.3f' % rec)\n",
        "print('Accuracy: %0.3f' % acc)\n",
        "print('F1 score: %0.3f' % f1)\n",
        "# print('AUC: %0.3f' % auc)"
      ],
      "metadata": {
        "id": "z_FPqhiPdfoW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a099a9-6403-4c32-cbaa-d213d1a8a26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.333\n",
            "Recall: 0.500\n",
            "Accuracy: 0.438\n",
            "F1 score: 0.444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM Classifier"
      ],
      "metadata": {
        "id": "zMPrlHkh5_y6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azNbCgEpjGKJ"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "lsvc = LinearSVC(maxIter=10, \\\n",
        "                 regParam=0.1, \\\n",
        "                 featuresCol=\"features\", \\\n",
        "                 labelCol=\"label\")\n",
        "\n",
        "lsvcModel = lsvc.fit(train)\n",
        "predictions_lsvc = lsvcModel.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "Mevaluator_lsvc = MulticlassClassificationEvaluator()\n",
        "Mevaluator_lsvc.setPredictionCol(\"prediction\")\n",
        "acc_lsvc = Mevaluator_lsvc.evaluate(predictions_lsvc, {Mevaluator_lsvc.metricName: \"accuracy\"})\n",
        "f1_lsvc = Mevaluator_lsvc.evaluate(predictions_lsvc, {Mevaluator_lsvc.metricName: \"f1\"})\n",
        "pre_lsvc = Mevaluator_lsvc.evaluate(predictions_lsvc, {Mevaluator_lsvc.metricName: \"precisionByLabel\"})\n",
        "rec_lsvc = Mevaluator_lsvc.evaluate(predictions_lsvc, {Mevaluator_lsvc.metricName: \"recallByLabel\"})\n",
        "# auc_lsvc = Mevaluator_lsvc.evaluate(predictions_lsvc, {Mevaluator_lsvc.metricName: \"areaUnderROC\"})\n",
        "\n",
        "print('Precision: %0.3f' % pre_lsvc)\n",
        "print('Recall: %0.3f' % rec_lsvc)\n",
        "print('Accuracy: %0.3f' % acc_lsvc)\n",
        "print('F1 score: %0.3f' % f1_lsvc)\n",
        "# print('AUC: %0.3f' % auc_lsvc)"
      ],
      "metadata": {
        "id": "Gx0AyS1Pgqfm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "527e58df-f943-461e-8c62-e817de189cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.375\n",
            "Recall: 0.500\n",
            "Accuracy: 0.500\n",
            "F1 score: 0.508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier"
      ],
      "metadata": {
        "id": "YgB53qR-6CrO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZBPOdx3Dztf"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
        "rfModel = rf.fit(train)\n",
        "predictions_rf = rfModel.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "Mevaluator_rf = MulticlassClassificationEvaluator()\n",
        "Mevaluator_rf.setPredictionCol(\"prediction\")\n",
        "acc_rf = Mevaluator_rf.evaluate(predictions_rf, {Mevaluator_rf.metricName: \"accuracy\"})\n",
        "f1_rf = Mevaluator_rf.evaluate(predictions_rf, {Mevaluator_rf.metricName: \"f1\"})\n",
        "pre_rf = Mevaluator_rf.evaluate(predictions_rf, {Mevaluator_rf.metricName: \"precisionByLabel\"})\n",
        "rec_rf = Mevaluator_rf.evaluate(predictions_rf, {Mevaluator_rf.metricName: \"recallByLabel\"})\n",
        "# auc_rf = Mevaluator_rf.evaluate(predictions_rf, {Mevaluator_rf.metricName: \"areaUnderROC\"})\n",
        "\n",
        "\n",
        "print('Precision: %0.3f' % pre_rf)\n",
        "print('Recall: %0.3f' % rec_rf)\n",
        "print('Accuracy: %0.3f' % acc_rf)\n",
        "print('F1 score: %0.3f' % f1_rf)\n",
        "# print('AUC: %0.3f' % auc_rf)"
      ],
      "metadata": {
        "id": "ydLr7T23pQIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "705f85b0-b6cb-4f19-d055-bc992b838882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.333\n",
            "Recall: 0.500\n",
            "Accuracy: 0.438\n",
            "F1 score: 0.444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qoRokqkaU1fZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}